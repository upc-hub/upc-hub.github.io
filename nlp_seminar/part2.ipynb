{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d6d5f2",
   "metadata": {},
   "source": [
    "# üìñ Part 2: Tokenization & Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3033b9a7",
   "metadata": {},
   "source": [
    "In this section, we will explore two fundamental steps in NLP:\n",
    "\n",
    "1. **Tokenization**: Splitting text into smaller units.\n",
    "2. **Embeddings**: Representing text numerically for machine learning models.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc5ee7",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Tokenization\n",
    "Tokenization is the process of breaking text into smaller units called tokens. These tokens can be words, subwords, or characters.\n",
    "\n",
    "### Types of Tokenization:\n",
    "- Word Tokenization\n",
    "- Subword Tokenization (e.g., Byte Pair Encoding)\n",
    "- Character Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc66824",
   "metadata": {},
   "source": [
    "### üíª Example: Word Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenization is a crucial step in NLP!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74ac26",
   "metadata": {},
   "source": [
    "___\n",
    "## üìê Embeddings\n",
    "Embeddings are numerical representations of words or texts in a continuous vector space. They help capture semantic relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ff37a",
   "metadata": {},
   "source": [
    "### Popular Embedding Methods:\n",
    "- One-Hot Encoding\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText\n",
    "- Transformer-based Embeddings (e.g., BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292f16d",
   "metadata": {},
   "source": [
    "### üíª Example: Using Pre-trained Word Embeddings with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f912d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Natural Language Processing\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}, Vector (first 5 dims): {token.vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83b76a",
   "metadata": {},
   "source": [
    "___\n",
    "## ‚úÖ Next Steps\n",
    "Proceed to Part 3: Fine-Tuning LLMs to learn how to adapt large language models for specific NLP tasks.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
